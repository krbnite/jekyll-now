{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is motivated by the paper, [A Systematic Approach for Variable Selection\n",
    "With Random Forests: Achieving Stable\n",
    "Variable Importance Values](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038868).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-22 23:43:22--  https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names\n",
      "Resolving archive.ics.uci.edu... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6816 (6.7K) [application/x-httpd-php]\n",
      "Saving to: 'agaricus-lepiota.names'\n",
      "\n",
      "agaricus-lepiota.na 100%[===================>]   6.66K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-09-22 23:43:24 (43.0 MB/s) - 'agaricus-lepiota.names' saved [6816/6816]\n",
      "\n",
      "--2019-09-22 23:43:24--  https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "Resolving archive.ics.uci.edu... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 373704 (365K) [application/x-httpd-php]\n",
      "Saving to: 'agaricus-lepiota.data'\n",
      "\n",
      "agaricus-lepiota.da 100%[===================>] 364.95K   708KB/s    in 0.5s    \n",
      "\n",
      "2019-09-22 23:43:25 (708 KB/s) - 'agaricus-lepiota.data' saved [373704/373704]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Table Data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names\n",
    "# Get Table MetaData\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
    "# Extract Feature Names \n",
    "!cat agaricus-lepiota.names | grep \"^[[:space:]]\\{4,5\\}[0-9]\\{1,2\\}.*:\" > agaricus-lepiota-feature-names.txt\n",
    "# Move to _data folder (put _data in .gitignore file)\n",
    "!mv agaricus* _data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've worked with the UCI ML repository's mushroom data set [before](https://krbnite.github.io/Treebeard-and-the-Fungus-Amongus/), and I shamelessly borrow my code from there to import, transform, and split \n",
    "the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Some Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_selection as filt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features names\n",
    "with open('_data/agaricus-lepiota-feature-names.txt') as f:\n",
    "    features = [line.split()[1][0:-1] for line in f]\n",
    "    \n",
    "# Get Data\n",
    "data = pd.read_csv('_data/agaricus-lepiota.data', names=['deadly']+features)\n",
    "y = pd.DataFrame([1 if target=='p' else 0 for target in data['deadly']], columns=['deadly'])\n",
    "x = data.drop('deadly', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train, Validate, Test\n",
    "x_trn, x_vt, y_trn, y_vt = train_test_split(x, y, train_size=0.70)\n",
    "x_val, x_tst, y_val, y_tst = train_test_split(x_vt, y_vt, test_size=0.50)\n",
    "\n",
    "# Some/Most techniques require the categorical vars to be one-hot encoded\n",
    "x_trn_code = pd.get_dummies(x_trn)\n",
    "x_val_code = pd.get_dummies(x_val)\n",
    "x_tst_code = pd.get_dummies(x_tst)\n",
    "\n",
    "# Make sure the various dummy vars are represented in each subset\n",
    "all_ftrs = pd.get_dummies(x).columns\n",
    "for ftr in all_ftrs.difference(x_trn_code.columns): x_trn_code[ftr]=0\n",
    "for ftr in all_ftrs.difference(x_val_code.columns): x_val_code[ftr]=0\n",
    "for ftr in all_ftrs.difference(x_tst_code.columns): x_tst_code[ftr]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Behnamian paper, they do not toggle many hyperparameters:\n",
    "* Leave mtry as default (sqrt(F))\n",
    "* If you choose some kind of additional constraints, keep them fixed for all runs\n",
    "\n",
    "Importantly, the authors looked at how many training iterations (\"retrains\" henceforth) it took \n",
    "for the average variable importance rankings to stabilize given RFs with 50, 200, 500, and 10k \n",
    "trees.  For each RF, they ran 25 retrains.\n",
    "\n",
    "They looked at both Gini important (aka mean decrease in Gini impurity, MDG) and permutation\n",
    "important (aka mean decrease in accuracy, MDA).\n",
    "\n",
    "Basically, they look at sequences like\n",
    "$$\\{\\overline{VI}_{p,nTree}(i)\\}_{i=1}^{25}$$\n",
    "\n",
    "where p is a fixed predictor, nTree is a fixed forest size, and the \n",
    "ith term is defined as\n",
    "\n",
    "$$\\overline{VI}_{p,nTree}(i) = \\frac{1}{i}\\sum\\limits_{j=1}^{i}{VI_{p,nTree}(j)}$$\n",
    "\n",
    "Plotting this sequence for a fixed predictor and forest size can shed light on how many\n",
    "runs one should do to stabilize the variable importances.  Better, one might create a \n",
    "\"spaghetti plot\" of these sequences for, say, the top 10 predictors from the first training\n",
    "iteration, then look to see when they all stop criss-crossing and seem to settle to a fixed\n",
    "value.  \n",
    "\n",
    "To have an aggregate indicator of stabilization over all predictors, the authors concocted\n",
    "a distance metric:\n",
    "\n",
    "$$D(i) = \\sqrt{\\frac{\\sum_{p=1}^{P}(\\overline{VI}_{p,nTree}(i) - \\overline{VI}_{p,nTree}(25))^{2}}{P}}$$\n",
    "\n",
    "This serves as an average \"distance from the true mean\" over all predictors, where the \"true mean\"\n",
    "is assumed to be well approximated by the average over 25 retrains.\n",
    "\n",
    "Their major finding was that a 10k-tree forest basically only needs one training iteration for\n",
    "variable importance covergence, but that it is computationally burdensome compared to averaging, say,\n",
    "the variable importances of a 50-tree forest over 21 retrains.  They quoted the timings for \n",
    "their data set, showing that the 10k-tree forest took 10x as long to train than the smaller \n",
    "forest over 21 retains.  \n",
    "\n",
    "This is interesting because it really calls into question what you want to do\n",
    "and which forest is best.  I've seen prediction accuracy stabilize at 50-100 trees,\n",
    "while variable importances fluctuate like crazy from one training iteration to the\n",
    "next w/ no change in hyperparameters.  This tells you that there are many ways to\n",
    "skin a cat (that is, many ways to effectively separate classes), but that for a \n",
    "given predictor set you might imagine there is one \"true way\".  In the \"true way\" sense,\n",
    "it \"feels\" like we should want to 10k-tree forest, but from a predictive accuracy, training\n",
    "time, and prediction time sense, it is better to go with a smaller forest.  Maybe the best\n",
    "of both worlds is finding the \"true rankings\", cutting the worst performers while maintaining\n",
    "the same level of accuracy, then using as big a forest as convenient for the final forest.\n",
    "\n",
    "For example, say you start with 100 predictors.  You find the true rankings using 30 retrains\n",
    "of a fairly small forest.  Then you cut predictors from the bottom until the accuracy starts\n",
    "to change.  Say after this, you have 35 predictors left.  At this point, opt for a slightly\n",
    "bigger forest than you initially would have (e.g., with 100 predictors, you might have preferred\n",
    "a 1k-tree forest, but could only afford a 500-tree forest in memory; well, go ahead and see\n",
    "if you can afford that 1k-tree forest now, baby!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

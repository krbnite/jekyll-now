---
title: AI at the Edge (Intel Scholarship)
layout: post
tags: ai edge easi
---

The term "edge" is funny in a way because it's literally defined as "local computing", or
"computing done nearby, not in the cloud."  10 years ago this was just called "doing computer
stuff."  

Jokes aside, despite the above having some truth to it, "edge computing" is usually 
reserved for "doing computer stuff" on
devices that aren't exactly mainstream computers, e.g., on a smarthome security camera,  a
smart refrigerator, or a wearable device -- platforms that typically have constraints 
that laptops or tablets don't have (low storage, limited processing power, low memory,
limited energy, lack of a screen, etc).  

For example, whenever you ask Siri or Alexa a question on your smartphone, TV remote, 
or speaker next to your bed, that question gets catapulted
into the cloud and the answer hurdles back down to your device.  

The gist is that a lot of these things (i) didn't exist before the cloud and (ii) relied
on cloud computing resources when they came into existence.  The cloud is great, but
it might not always be available (network issues) or the back-and-forth between the
device and the cloud might too laggy (latency issues).  

For example, let's say
you put a video camera on the top of Mount Everest that can only hold up to 24 hours
of video.  The objective is to identify and store the footage of a bar-headed 
goose whenever one flies within the camera's field of view.  However, the video camera is not
connected to a WiFi and doesn't have enough power to transmit data to overhead spacecraft,
so you must collect it in person once a month.  What do you do?  Well, for one,
 use the simplest, most efficient CNN you can onboard (remember, we are low on storage,
memory, energy, and compute power).  This is edge computing!  Doing this, we need only store 
those intervals of time that positively classify as a bar-headed goose sigting.  No 
cloud needed.

Another example: wearables data.  Wouldn't it be nice if you could have a digital twin
of yourself to periodically inspect?  A virtual emodiment of your biological state: your
heart rate, respiration rate, anxiety levels, fitness level, and so on, there to 
investigate and lend a quantitative understanding.  With the sensors onboard wearables, this
is possible, but the current trend is in cloud computing: the devices measure this
data, stream it to your mobile phone via Bluetooth, which streams it to the cloud for
analysis and storage.  But what if you don't want all this information about you
shared with some corporate entity, or stored on some server waiting to be hacked?  And
how about when wearables start gaining more sensitive insights into your health, and your
physical and psychological state?  The issue of sharing this data becomes more
troublesome.  Edge computing is the answer!

Finally, how about self-driving cars: are you comfortable with the increased latency involved
with sending data back and forth, to and from the cloud?  If an autonomous vehicle is about to hit
a person in the middle of the road, wouldn't it be nice to know it can make a split-second
decision on the fly -- even out in the middle of nowhere, where there is no cloud in sight?!

You get the idea!  For low-resource devices, the
cloud is often the only option -- but things are changing.  


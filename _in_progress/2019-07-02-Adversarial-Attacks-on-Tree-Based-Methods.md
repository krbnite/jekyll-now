

You hear all the time about adversarial attacks on neural networks.

In one story, all an attacker would have to do is put a glossy sticker on a street sign in the right way,
and -- BAM! -- classification power destroyed.  

Another example: adding just amount of "directed noise" to an image to change its classification in a controlled
manner.

However, at least in my personal coordinate system, it doesn't seem like many people discuss adversarial attacks on 
other methods, like random forests.

This post is a quick one-two about that...

http://proceedings.mlr.press/v97/chen19m/chen19m.pdf

https://www.classes.cs.uchicago.edu/archive/2018/fall/23200-1/23.pdf

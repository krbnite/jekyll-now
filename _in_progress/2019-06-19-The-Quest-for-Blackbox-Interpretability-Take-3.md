---
title: The Quest for Blackbox Interpretability (Take 3, LIME)
layout: post
tags: machine-learning random-forest predictive-modeling model-interpretability easi
---


* [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)
  - Original LIME paper
* [Justifying a Random Forest](https://roywright.me/2018/02/09/justifying-random-forest/)
  - Shows an example using LIME
  - Describes some shortcomings of LIME
  - Plugs `treeinterpreter` Python package
  - Shows how he made `treeinterpreter` better / more robust
* [Model-Agnostic Methods: Local Surrogate (LIME)](https://christophm.github.io/interpretable-ml-book/lime.html)
  - From the online book "[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)"

Crazily enough, I just got an email from Two Sigma with an article that covers exactly the topics I've reading
and writing about for the past week or so:
* https://www.twosigma.com/insights/article/interpretability-methods-in-machine-learning-a-brief-survey



Related:  I saw that there is a Kaggle micro-course on model explainability.  I should check it out:
* https://towardsdatascience.com/why-model-explainability-is-the-next-data-science-superpower-b11b6102a5e0
* https://www.kaggle.com/learn/machine-learning-explainability?utm_medium=blog&utm_source=medium&utm_campaign=medium-learn-explain
